{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class NeuralAccumulatorCell(nn.Module):\n",
    "    \n",
    "    # Feed forward but Weight decomposition\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.W_hat = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.M_hat = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.W = Parameter(torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat))\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "        init.kaiming_uniform_(self.W_hat, a=math.sqrt(5))\n",
    "        init.kaiming_uniform_(self.M_hat, a=math.sqrt(5))\n",
    "        \n",
    "        #init.normal_(self.W_hat)\n",
    "        #init.normal_(self.M_hat)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.W, self.bias)\n",
    "\n",
    "\n",
    "class NAC(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims):\n",
    "        '''\n",
    "        dims = [input_dim + hidden_dims + output_dims]\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_layers = len(dims) - 1\n",
    "        \n",
    "        layers = nn.ModuleList()\n",
    "        layers.extend([NeuralAccumulatorCell(dims[i],dims[i+1]) for i in range(self.num_layers)])\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "class NeuralArithmeticLogicUnitCell(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.eps = 1e-10\n",
    "\n",
    "        self.G = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.W = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.register_parameter('bias', None)\n",
    "        self.nac = NeuralAccumulatorCell(in_dim, out_dim)\n",
    "\n",
    "        init.kaiming_uniform_(self.G, a=math.sqrt(5))\n",
    "        init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        a = self.nac(input)\n",
    "        g = torch.sigmoid(F.linear(input, self.G, self.bias))\n",
    "        add_sub = g * a\n",
    "        log_input = torch.log(torch.abs(input) + self.eps)\n",
    "        m = torch.exp(self.nac(log_input))\n",
    "        # m = torch.exp(F.linear(log_input, self.W, self.bias))\n",
    "        mul_div = (1 - g) * m\n",
    "        y = add_sub + mul_div\n",
    "        return y\n",
    "\n",
    "\n",
    "class NALU(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(dims) - 1\n",
    "        layers = nn.ModuleList()\n",
    "        layers.extend([NeuralArithmeticLogicUnitCell(dims[i],dims[i+1]) for i in range(self.num_layers)])\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "class NALU_LSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.i2h = nn.Sequential(\n",
    "            nn.Linear(input_size, 4 * hidden_size, bias=bias),\n",
    "#             nn.BatchNorm1d(4 * hidden_size),\n",
    "#             nn.LeakyReLU(0.2,inplace=True),\n",
    "#             nn.Linear(4 *input_size, 4 * hidden_size, bias=bias),\n",
    "        )\n",
    "        self.h2h = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size, bias=bias),\n",
    "#            nn.BatchNorm1d(4 * hidden_size),\n",
    "#             nn.LeakyReLU(0.2,inplace=True),\n",
    "#             nn.Linear(4 * hidden_size, 4 * hidden_size, bias=bias)\n",
    "        )\n",
    "        self.nalu_h = NALU([hidden_size, hidden_size])\n",
    "        self.nalu_c = NALU([hidden_size, hidden_size])\n",
    "        self.out = nn.Linear(hidden_size, input_size, bias=bias)\n",
    "        self.apply(self.weight_init)\n",
    "\n",
    "    def weight_init(self,m):\n",
    "\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for name, w in m.named_parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x, hidden = None):\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = x.new_zeros(x.size(0), self.hidden_size, requires_grad=False)\n",
    "            hidden = (hidden, hidden)\n",
    "            \n",
    "        h, c = hidden\n",
    "        \n",
    "        preact = self.i2h(x) + self.h2h(h)\n",
    "        \n",
    "        # First: apply nalu to replace activation func\n",
    "        \n",
    "        # self.nalu(preact)\n",
    "        \n",
    "        gates = preact[:, :3 * self.hidden_size].sigmoid()\n",
    "        g_t = preact[:, 3 * self.hidden_size:].tanh()\n",
    "        i_t = gates[:, :self.hidden_size] \n",
    "        f_t = gates[:, self.hidden_size:2 * self.hidden_size]\n",
    "        o_t = gates[:, -self.hidden_size:]\n",
    "        \n",
    "        # Second: Apply it in the output and hidden layer\n",
    "\n",
    "        c_t = (c*f_t) + (i_t*g_t)\n",
    "\n",
    "        h_t = o_t * c_t.tanh()\n",
    "        \n",
    "        # return x, (h_t, c_t) # LSTM\n",
    "        \n",
    "        # return x + self.out(h_t), (h_t + h, c_t + c) # Residule LSTM\n",
    "\n",
    "        return  x + self.out(h_t) , (self.nalu_h(h_t + h), self.nalu_c(c_t + c)) # Residule NALU\n",
    "\n",
    "class NALU_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_sizes, bidirectional = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.num_dir = 2\n",
    "        else:\n",
    "            self.num_dir = 1\n",
    "            \n",
    "        self.input_size = input_size\n",
    "        self.L = len(hidden_sizes)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend([NALU_LSTMCell(input_size,i) for i in hidden_sizes])\n",
    "        self.c0 = nn.ParameterList([nn.Parameter(torch.randn(self.num_dir, 1,i)) for i in hidden_sizes])\n",
    "        self.h0 = nn.ParameterList([nn.Parameter(torch.randn(self.num_dir, 1,i)) for i in hidden_sizes])\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        '''\n",
    "        input_shape = B, S, input_size\n",
    "        output_shape = B, num_dir, L, S, input_size\n",
    "        hidden, cells = S * (B, hidden_dim)\n",
    "        '''\n",
    "        \n",
    "        B,S = input.shape[:-1]\n",
    "        \n",
    "        outputs = torch.zeros(B, self.num_dir, self.L+1, S, self.input_size)\n",
    "        outputs[:,:,0,:,:] = input.unsqueeze(1).expand_as(outputs[:,:,0,:,:])\n",
    "        hiddens = []\n",
    "        cells = []\n",
    "    \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            f_h, f_c = self.h0[i][0].repeat(B,1), self.c0[i][0].repeat(B,1)\n",
    "            if self.bidirectional:\n",
    "                i_h, i_c = self.h0[i][1].repeat(B,1), self.c0[i][1].repeat(B,1)\n",
    "            for j in range(S):\n",
    "                f_out, (f_h,f_c) = layer(outputs[:,0,i,j,:].clone(), (f_h,f_c))\n",
    "                outputs[:,0,i+1,j,:] = f_out\n",
    "\n",
    "            if self.bidirectional:\n",
    "                for j in reversed(range(S)):\n",
    "                    i_out, (i_h,i_c) = layer(outputs[:,1,i,j,:].clone(), (i_h,i_c))\n",
    "                    outputs[:,1,i+1,j,:] = i_out\n",
    "                hiddens.append((torch.stack([f_h,i_h])))\n",
    "                cells.append(torch.stack([f_c,i_c]))\n",
    "            else:\n",
    "                hiddens.append(f_h)\n",
    "                cells.append(f_c)\n",
    "                \n",
    "        \n",
    "        return outputs[:,:,1:,:,:].contiguous(), (hiddens, cells)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
